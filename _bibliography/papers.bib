@inproceedings{paull2016unified,
  title={A Unified Resource-Constrained Framework for Graph SLAM},
  author={Paull, Liam and Huang, Guoquan and Leonard, John J},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1--8},
  month={May},
  year={2016},
  image = {papers/paull2016unified.png},
  arxiv = {http://liampaull.ca/publications/Paull_ICRA_2016.pdf},
  code = {https://github.com/liampaull/Resource_Constrained},
  slides = {http://liampaull.ca/publications/Paull_ICRA_2016_presentation.pptx},
  poster = {http://liampaull.ca/publications/Paull_ICRA_2016_poster.pptx},
  abstract = {Graphical methods have proven an extremely useful tool employed by the mobile robotics community to frame estimation problems. Incremental solvers are able to process incoming sensor data and produce maximum a posteriori (MAP) estimates in realtime by exploiting the natural sparsity within the graph for reasonable-sized problems. However, to enable truly longterm operation in prior unknown environments requires algorithms whose computation, memory, and bandwidth (in the case of distributed systems) requirements scale constantly with time and environment size. Some recent approaches have addressed this problem through a two-step process - first the variables selected for removal are marginalized which induces density, and then the result is sparsified to maintain computational efficiency. Previous literature generally addresses only one of these two components. In this work, we attempt to explicitly connect all of the aforementioned resource constraint requirements by considering the node removal and sparsification pipeline in its entirety. We formulate the node selection problem as a minimization problem over the penalty to be paid in the resulting sparsification. As a result, we produce node subset selection strategies that are optimal in terms of minimizing the impact, in terms of Kullback-Liebler divergence (KLD), of approximating the dense distribution by a sparse one. We then show that one instantiation of this problem yields a computationally tractable formulation. Finally, we evaluate the method on standard datasets and show that the KLD is minimized as compared to other commonly-used heuristic node selection techniques.},
}

@inproceedings{mu2016iros,
  title={Slam with objects using a nonparametric pose graph},
  author={Mu, Beipeng and Liu, Shih-Yuan and Paull, Liam and Leonard, John and How, Jonathan P},
  booktitle={IEEE/RSJ International Conference onnIntelligent Robots and Systems (IROS)},
  year={2016},
  month={Oct},
  image = {papers/mu2016iros.png},
  arxiv = {1704.05959},
  video = {https://www.youtube.com/watch?v=gOwMiFlj8KU},
  abstract = {Mapping and self-localization in unknown environments are fundamental capabilities in many robotic applications. These tasks typically involve the identification of objects as unique features or landmarks, which requires the objects both to be detected and then assigned a unique identifier that can be maintained when viewed from different perspectives and in different images. The data association and simultaneous localization and mapping (SLAM) problems are, individually, well-studied in the literature. But these two problems are inherently tightly coupled, and that has not been well-addressed. Without accurate SLAM, possible data associations are combinatorial and become intractable easily. Without accurate data association, the error of SLAM algorithms diverge easily. This paper proposes a novel nonparametric pose graph that models data association and SLAM in a single framework. An algorithm is further introduced to alternate between inferring data association and performing SLAM. Experimental results show that our approach has the new capability of associating object detections and localizing objects at the same time, leading to significantly better performance on both the data association and SLAM problems than achieved by considering only one and ignoring imperfections in the other.},
}

@inproceedings{paull2017duckietown,
  title={Duckietown: an open, inexpensive and flexible platform for autonomy education and research},
  author={Paull, Liam and Tani, Jacopo and Ahn, Heejin and Alonso-Mora, Javier and Carlone, Luca and Cap, Michal and Chen, Yu Fan and Choi, Changhyun and Dusek, Jeff and Fang, Yajun and others},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1497--1504},
  year={2017},
  month={May},
  image = {papers/paull2017duckietown.png},
  arxiv = {http://www.mit.edu/~hangzhao/papers/duckietown.pdf},
  abstract = {Duckietown is an open, inexpensive and flexible platform for autonomy education and research. The platform comprises small autonomous vehicles (“Duckiebots”) built from off-the-shelf components, and cities (“Duckietowns”) complete with roads, signage, traffic lights, obstacles, and citizens (duckies) in need of transportation. The Duckietown platform offers a wide range of functionalities at a low cost. Duckiebots sense the world with only one monocular camera and perform all processing onboard with a Raspberry Pi 2, yet are able to: follow lanes while avoiding obstacles, pedestrians (duckies) and other Duckiebots, localize within a global map, navigate a city, and coordinate with other Duckiebots to avoid collisions. Duckietown is a useful tool since educators and researchers can save money and time by not having to develop all of the necessary supporting infrastructure and capabilities. All materials are available as open source, and the hope is that others in the community will adopt the platform for education and research.},
}

@inproceedings{rosman2017hybrid,
  title={Hybrid control and learning with coresets for autonomous vehicles},
  author={Rosman, Guy and Paull, Liam and Rus, Daniela},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={6894--6901},
  year={2017},
  month={Oct},
  arxiv = {http://people.csail.mit.edu/rosman/papers/ctrl_embedding.pdf},
  image = {papers/rosman2017hybrid.png},
  abstract = {Modern autonomous systems such as driverless vehicles need to safely operate in a wide range of conditions. A potential solution is to employ a hybrid systems approach, where safety is guaranteed in each individual mode within the system. This offsets complexity and responsibility from the individual controllers onto the complexity of determining discrete mode transitions. In this work we propose an efficient framework based on recursive neural networks and coreset data summarization to learn the transitions between an arbitrary number of controller modes that can have arbitrary complexity. Our approach allows us to efficiently gather annotation data from the large-scale datasets that are required to train such hybrid nonlinear systems to be safe under all operating conditions, favoring underexplored parts of the data. We demonstrate the construction of the embedding, and efficient detection of switching points for autonomous and nonautonomous car data. We further show how our approach enables efficient sampling of training data, to further improve either our embedding or the controllers},
}

@inproceedings{paull2018probabilistic,
  title={Probabilistic cooperative mobile robot area coverage and its application to autonomous seabed mapping},
  author={Paull, Liam and Seto, Mae and Leonard, John J and Li, Howard},
  journal={The International Journal of Robotics Research},
  volume={37},
  number={1},
  pages={21--45},
  year={2018},
  image = {papers/paull2018coverage.png},
  abstract = {There are many applications that require mobile robots to autonomously cover an entire area with a sensor or end effector. The vast majority of the literature on this subject is focused on addressing path planning for area coverage under the assumption that the robot’s pose is known or that error is bounded. In this work, we remove this assumption and develop a completely probabilistic representation of coverage. We show that coverage is guaranteed as long as the robot pose estimates are consistent, a much milder assumption than zero or bounded error. After formally connecting robot sensor uncertainty with area coverage, we propose an adaptive sliding window filter pose estimator that provides a close approximation to the full maximum a posteriori estimate with a computation cost that is bounded over time. Subsequently, an adaptive planning strategy is presented that automatically exploits conditions of low vehicle uncertainty to more efficiently cover an area. We further extend this approach to the multi-robot case where robots can communicate through a (possibly faulty and low-bandwidth) channel and make relative measurements of one another. In this case, area coverage is achieved more quickly since the uncertainty over the robots’ trajectories is reduced. We apply the framework to the scenario of mapping an area of seabed with an autonomous underwater vehicle. Experimental results support the claim that our method achieves guaranteed complete coverage notwithstanding poor navigational sensors and that resulting path lengths required to cover the entire area are shortest using the proposed cooperative and adaptive approach.},
}

@inproceedings{ort2018icra,
  title={Autonomous Vehicle Navigation in Rural Environments without Detailed Prior Maps},
  author={Ort, Teddy and Paull, Liam and Rus, Daniela},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2018},
  month={May},
  image = {papers/ort2018icra.png},
  arxiv = {https://toyota.csail.mit.edu/sites/default/files/documents/papers/ICRA2018_AutonomousVehicleNavigationRuralEnvironment.pdf},
  abstract = {State-of-the-art autonomous driving systems rely heavily on detailed and highly accurate prior maps. However, outside of small urban areas, it is very challenging to build, store, and transmit detailed maps since the spatial scales are so large. Furthermore, maintaining detailed maps of large rural areas can be impracticable due to the rapid rate at which these environments can change. This is a significant limitation for the widespread applicability of autonomous driving technology, which has the potential for an incredibly positive societal impact. In this paper, we address the problem of autonomous navigation in rural environments through a novel mapless driving framework that combines sparse topological maps for global navigation with a sensor-based perception system for local navigation. First, a local navigation goal within the sensor view of the vehicle is chosen as a waypoint leading towards the global goal. Next, the local perception system generates a feasible trajectory in the vehicle frame to reach the waypoint while abiding by the rules of the road for the segment being traversed. These trajectories are updated to remain in the local frame using the vehicle’s odometry and the associated uncertainty based on the least-squares residual and a recursive filtering approach, which allows the vehicle to navigate road networks reliably, and at high speed, without detailed prior maps. We demonstrate the performance of the system on a full-scale autonomous vehicle navigating in a challenging rural environment and benchmark the system on a large amount of collected data.},
}

@inproceedings{mai2018local,
  title={Local Positioning System Using UWB Range Measurements for an Unmanned Blimp},
  author={Mai, Vincent and Kamel, Mina and Krebs, Matthias and Schaffner, Andreas and Meier, Daniel and Paull, Liam and Siegwart, Roland},
  journal={IEEE Robotics and Automation Letters},
  volume={3},
  number={4},
  pages={2971--2978},
  year={2018},
  month={October},
  image = {papers/mai2018blimp.png},
  arxiv = {https://ieeexplore.ieee.org/document/8392389},
  abstract = {Unmanned blimps are a safe and reliable alternative to conventional drones when flying above people. On-board real-time tracking of their pose and velocities is a necessary step toward autonomous navigation. There is a need for an easily deployable technology that is able to accurately and robustly estimate the pose and velocities of a blimp in 6 DOF, as well as unexpected applied forces and torques, in an uncontrolled environment. We present two multiplicative extended Kalman filters using ultrawideband radio sensors and a gyroscope to address this challenge. One filter is updated using a dynamics model of the blimp, whereas the other uses a constant speed model. We describe a set of experiments in which these estimators have been implemented on an embedded flight controller. They were tested and compared in accuracy and robustness in a hardware-in-loop simulation as well as on a real blimp. This approach can be generalized to any lighter than air robot to track it with the necessary accuracy, precision, and robustness to allow autonomous navigation.},
}

@inproceedings{CTCNet,
  title = {Geometric Consistency for Self-Supervised End-to-End Visual Odometry},
  author = {Iyer, Ganesh and Murthy, J Krishna and Gunshi Gupta, K and Paull, Liam},
  booktitle = {CVPR Workshop on Deep Learning for Visual SLAM},
  month = {June},
  year = {2018},
  arxiv = {1804.03789},
  projectpage = {https://krrish94.github.io/CTCNet-release/},
  image = {papers/ctcnet.png},
  abstract = {With the success of deep learning based approaches in tackling challenging problems in computer vision, a wide range of deep architectures have recently been proposed for the task of visual odometry (VO) estimation. Most of these proposed solutions rely on supervision, which requires the acquisition of precise ground-truth camera pose information, collected using expensive motion capture systems or high-precision IMU/GPS sensor rigs. In this work, we propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose "Composite Transformation Constraints (CTCs)", that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate. We also present a method of characterizing the uncertainty in VO estimates thus obtained. To evaluate our VO pipeline, we present exhaustive ablation studies that demonstrate the efficacy of end-to-end, self-supervised methodologies to train deep models for monocular VO. We show that leveraging concepts from geometry and incorporating them into the training of a recurrent neural network results in performance competitive to supervised deep VO methods.},
}

@inproceedings{amini2018learning,
  title={Learning steering bounds for parallel autonomous systems},
  author={Amini, Alexander and Paull, Liam and Balch, Thomas and Karaman, Sertac and Rus, Daniela},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2018},
  month={May},
  arxiv = {https://dspace.mit.edu/handle/1721.1/117632},
  image = {papers/amini2018parallel.png},
  abstract = {Deep learning has been successfully applied to “end-to-end” learning of the autonomous driving task, where a deep neural network learns to predict steering control commands from camera data input. While these previous works support reactionary control, the representation learned is not usable for higher-level decision making required for autonomous navigation. This paper tackles the problem of learning a representation to predict a continuous control probability distribution, and thus steering control options and bounds for those options, which can be used for autonomous navigation. Each mode of the distribution encodes a possible macro-action that the system could execute at that instant, and the covariances of the modes place bounds on safe steering control values. Our approach has the added advantage of being trained on unlabeled data collected from inexpensive cameras. The deep neural network based algorithm generates a probability distribution over the space of steering angles, from which we leverage Variational Bayesian methods to extract a mixture model and compute the different possible actions in the environment. A bound, which the autonomous vehicle must respect in our parallel autonomy setting, is then computed for each of these actions. We evaluate our approach on a challenging dataset containing a wide variety of driving conditions, and show that our algorithm is capable of parameterizing Gaussian Mixture Models for possible actions, and extract steering bounds with a mean error of only 2 degrees. Additionally, we demonstrate our system working on a full scale autonomous vehicle and evaluate its ability to successful handle various different parallel autonomy situations.},
}

@inproceedings{bharadhwaj2018data,
  title={A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies},
  author={Bharadhwaj, Homanga and Wang, Zihan and Bengio, Yoshua and Paull, Liam},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)},
  year={2019},
  month={May},
  arxiv = {1810.04871},
  image = {papers/homanga2019icra.png},
  abstract = {Learning effective visuomotor policies for robots purely from data is challenging, but also appealing since a learning-based system should not require manual tuning or calibration. In the case of a robot operating in a real environment the training process can be costly, time-consuming, and even dangerous since failures are common at the start of training. For this reason, it is desirable to be able to leverage \textit{simulation} and \textit{off-policy} data to the extent possible to train the robot. In this work, we introduce a robust framework that plans in simulation and transfers well to the real environment. Our model incorporates a gradient-descent based planning module, which, given the initial image and goal image, encodes the images to a lower dimensional latent state and plans a trajectory to reach the goal. The model, consisting of the encoder and planner modules, is trained through a meta-learning strategy in simulation first. We subsequently perform adversarial domain transfer on the encoder by using a bank of unlabelled but random images from the simulation and real environments to enable the encoder to map images from the real and simulated environments to a similarly distributed latent representation. By fine tuning the entire model (encoder + planner) with far fewer real world expert demonstrations, we show successful planning performances in different navigation tasks.},
}

@inproceedings{sai2019dal,
  title = {Deep Active Localization},
  author = {Krishna, Sai and Seo, Keehong and Bhatt, Dhaivat and Mai, Vincent and Murthy, Krishna and Paull, Liam},
  booktitle = {IEEE Robotics and Automation Letters (RAL)},
  year = {2019},
  month = {May},
  arxiv = {1903.01669},
  code = {https://github.com/montrealrobotics/dal},
  image = {papers/dal.png},
  abstract = {Active localization is the problem of generating robot actions that allow it to maximally disambiguate its pose within a reference map. Traditional approaches to this use an information-theoretic criterion for action selection and hand-crafted perceptual models. In this work we propose an end-to-end differentiable method for learning to take informative actions that is trainable entirely in simulation and then transferable to real robot hardware with zero refinement. The system is composed of two modules: a convolutional neural network for perception, and a deep reinforcement learned planning module. We introduce a multi-scale approach to the learned perceptual model since the accuracy needed to perform action selection with reinforcement learning is much less than the accuracy needed for robot control. We demonstrate that the resulting system outperforms using the traditional approach for either perception or planning. We also demonstrate our approaches robustness to different map configurations and other nuisance parameters through the use of domain randomization in training. The code is also compatible with the OpenAI gym framework, as well as the Gazebo simulator.},
}

@inproceedings{adr,
  title = {Active Domain Randomization},
  author = {Mehta, Bhairav and Diaz, Manfred and Golemo, Florian and Pal, Christopher and Paull, Liam},
  booktitle = {Conference on Robot Learning (CoRL)},
  year = {2019},
  arxiv = {1904.04762},
  code = {https://github.com/montrealrobotics/active-domainrand},
  image = {papers/adr.gif},
  abstract = {We tackle the uniform sampling assumption in domain randomization and learn a randomization strategy, looking for the most informative environments. Our method shows significant improvements in agent performance, agent generalization, sample complexity, and interpretability over the traditional domain and dynamics randomization strategies.},
}

@inproceedings{gradslam,
  title = {gradSLAM: Dense SLAM meets automatic differentiation},
  author = {{Krishna Murthy}, Jatavallabhula and Iyer, Ganesh and Paull, Liam},
  booktitle = {International Conference on Robotics and Automation (ICRA)},
  year={2020},
  arxiv = {1910.10672},
  projectpage = {https://gradslam.github.io},
  code = {https://github.com/gradslam/gradslam},
  image = {papers/gradslam.png},
  abstract = {The question of "representation" is central in the context of dense simultaneous localization and mapping (SLAM). Newer learning-based approaches have the potential to leverage data or task performance to directly inform the choice of representation. However, learning representations for SLAM has been an open question, because traditional SLAM systems are not end-to-end differentiable.In this work, we present gradSLAM, a differentiable computational graph take on SLAM. Leveraging the automatic differentiation capabilities of computational graphs, gradSLAM enables the design of SLAM systems that allow for gradient-based learning across each of their components, or the system as a whole. This is achieved by creating differentiable alternatives for each non-differentiable component in a typical dense SLAM system. Specifically, we demonstrate how to design differentiable trust-region optimizers, surface measurement and fusion schemes, as well as differentiate over rays, without sacrificing performance. This amalgamation of dense SLAM with computational graphs enables us to backprop all the way from 3D maps to 2D pixels, opening up new possibilities in gradient-based learning for SLAM.},
}

@inproceedings{lamaml,
  title = {La-MAML: Look-ahead Meta Learning for Continual Learning},
  author = {Gupta, Gunshi and Yadav, Karmesh and Paull, Liam},
  booktitle = {Neural Information Processing Systems (Neurips)},
  highlight = {Oral (top 1.1%)},
  year = {2020},
  arxiv = {2007.13904},
  projectpage = {https://mila.quebec/en/article/la-maml-look-ahead-meta-learning-for-continual-learning/},
  image = {papers/lamaml.png},
  abstract = {The continual learning problem involves training models with limited capacity to perform well on a set of an unknown number of sequentially arriving tasks. While meta-learning shows great potential for reducing interference between old and new tasks, the current training procedures tend to be either slow or offline, and sensitive to many hyper-parameters. In this work, we propose Look-ahead MAML (La-MAML), a fast optimisation-based meta-learning algorithm for online-continual learning, aided by a small episodic memory. Our proposed modulation of per-parameter learning rates in our meta-learning update allows us to draw connections to prior work on hypergradients and meta-descent. This provides a more flexible and efficient way to mitigate catastrophic forgetting compared to conventional prior-based methods. La-MAML achieves performance superior to other replay-based, prior-based and meta-learning based approaches for continual learning on real-world visual classification benchmarks.},
}

@inproceedings{che2020neurips,
  title = {Your GAN is Secretly an Energy-based Model and You Should Use Discriminator Driven Latent Sampling},
  author = {Che, Tong and Zhang, Ruixiang and Sohl-Dickstein, Jascha and Larochelle, Hugo and Paull, Liam and Cao, Yuan and Bengio, Yoshua},
  booktitle = {Neural Information Processing Systems (Neurips)},
  year = {2020},
  arxiv = {2003.06060},
  abstract = {We show that the sum of the implicit generator log-density of a GAN with the logit score of the discriminator defines an energy function which yields the true data density when the generator is imperfect but the discriminator is optimal, thus making it possible to improve on the typical generator. To make that practical, we show that sampling from this modified density can be achieved by sampling in latent space according to an energy-based model induced by the sum of the latent prior log-density and the discriminator output score. This can be achieved by running a Langevin MCMC in latent space and then applying the generator function, which we call Discriminator Driven Latent Sampling~(DDLS). We show that DDLS is highly efficient compared to previous methods which work in the high-dimensional pixel space and can be applied to improve on previously trained GANs of many types. We evaluate DDLS on both synthetic and real-world datasets qualitatively and quantitatively. On CIFAR-10, DDLS substantially improves the Inception Score of an off-the-shelf pre-trained SN-GAN from 8.22 to 9.09 which is even comparable to the class-conditional BigGAN model. This achieves a new state-of-the-art in unconditional image synthesis setting without introducing extra parameters or additional training.},
}

@inproceedings{mehta2020curriculum,
  title = {Curriculum in Gradient-Based Meta-Reinforcement Learning},
  author = {Mehta, Bhairav and Deleu, Tristan and Raparthy, {Sharath Chandra} and Pal, Christopher and Paull, Liam},
  booktitle = {BETR-RL Workshop},
  year = {2020},
  arxiv = {2002.07956},
  image = {papers/mehta2020curriculum.png},
  abstract = {Can Meta-RL use curriculum learning? In this work, we explore that question and find that curriculum learning stabilizes meta-RL in complex navigation and locomotion tasks. We also highlight issues with Meta-RL benchmarks by highlighting failure cases when we vary task distributions.},
}

@inproceedings{ssadr,
  title = {Generating Automatic Curricula via Self-Supervised Active Domain Randomization},
  author = {Raparthy, {Sharath Chandra} and Mehta, Bhairav and Paull, Liam},
  booktitle = {BETR-RL Workshop},
  year = {2020},
  arxiv = {2002.07911},
  code = {https://github.com/montrealrobotics/unsupervised-adr},
  image = {papers/ssadr.png},
  abstract = {Can you learn domain randomization curricula with no rewards? We show that agents trained via self-play in the ADR framework outperform uniform domain randomization by magnitudes in both simulated and real-world transfer.},
}

@inproceedings{aido2018,
  title = {The AI Driving Olympics at NeurIPS 2018},
  author = {Zilly, Julian and Tani, Jacopo and Considine, Breandan and Mehta, Bhairav and Daniele, {Andrea F} and Diaz, Manfred and Bernasconi, Gianmarco and Ruch, Claudio Hakenberg, Jan and Golemo, Florian and Bowser, {A Kirsten} and Walter, {Matthew R} and Hristov, Ruslan and Mallya, Sunil and Frazzoli, Emilio and Censi, Andrea and Paull, Liam},
  booktitle = {Springer},
  year = {2020},
  arxiv = {1903.02503},
  image = {papers/aido18.png},
  abstract = {Despite recent breakthroughs, the ability of deep learning and reinforcement learning to outperform traditional approaches to control physically embodied robotic agents remains largely unproven. To help bridge this gap, we created the “AI Driving Olympics” (AI-DO), a competition with the objective of evaluating the state of the art in machine learning and artificial intelligence for mobile robotics. Based on the simple and well specified autonomous driving and navigation environment called “Duckietown,” AI-DO includes a series of tasks of increasing complexity – from simple lane-following to fleet management. For each task, we provide tools for competitors to use in the form of simulators, logs, code templates, baseline implementations and low-cost access to robotic hardware. We evaluate submissions in simulation online, on standardized hardware environments, and finally at the competition event. The first AI-DO, AI-DO 1, occurred at the Neural Information Processing Systems (NeurIPS) conference in December 2018. The results of AI-DO 1 highlight the need for better benchmarks, which are lacking in robotics, as well as improved mechanisms to bridge the gap between simulation and reality.},
}

@inproceedings{probod,
  title = {Probabilistic Object Detection: Strenghts, Weaknesses, and Opportunities},
  author = {Bhatt, Dhaivat and Bansal, Dishank and Gupta, Gunshi and Jatavallabhula, {Krishna Murthy} and Lee, Hanju and Paull, Liam},
  booktitle = {ICML workshop on AI for autonomous driving},
  year = {2020},
  projectpage = {https://gunshigupta.netlify.app/publication/probod/},
  image = {papers/probod.png},
  abstract = {Deep neural networks are the de-facto standard for object detection in autonomous driving applications. However, neural networks cannot be blindly trusted even within the training data distribution, let alone outside it. This has paved way for several probabilistic object detection techniques that measure uncertainty in the outputs of an object detector. Through this position paper, we serve three main purposes. First, we briefly sketch the landscape of current methods for probabilistic object detection. Second, we present the main shortcomings of these approaches. Finally, we present promising avenues for future research, and proof-of-concept results where applicable. Through this effort, we hope to bring the community one step closer to performing accurate, reliable, and consistent probabilistic object detection.},
}


@inproceedings{biv,
  title = {Batch Inverse-Variance Weighting: Deep Heteroscedastic Regression},
  author = {Mai, Vincent and Khamies, Waleed and Paull, Liam},
  booktitle = {ICML Workshop on Uncertainty & Robustness in Deep Learning},
  year = {2021},
  arxiv = {2107.04497},
  abstract = {In the supervised learning task of heteroscedastic regression, each label is subject to noise from a different distribution. The label generator may estimate the variance of the noise distribution for each label, which is useful information to mitigate its impact. We adapt an inverse-variance weighted mean square error, based on the Gauss-Markov theorem, for gradient descent on neural networks. We introduce Batch Inverse-Variance, a loss function which is robust to near-ground truth samples, and allows to control the effective learning rate. Our experimental results show that BIV improves significantly the performance of the networks on two noisy datasets, compared to L2 loss, inverse-variance weighting, and a filtering-based baseline.},
}
